
  <!DOCTYPE html>
  <html lang="en">
    <head>
      <meta charset="utf-8">
      <meta http-equiv="X-UA-Compatible" content="IE=edge">
      <meta name="viewport" content="width=device-width, initial-scale=1">
      <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
      <meta name="description" content="">
      <meta name="author" content="">
      <link rel="icon" href="../../favicon.ico">

      <title>A prediction model you can count on</title>

      <!-- Bootstrap core CSS -->
      <link href="../static/css/bootstrap.min.css" rel="stylesheet">

      <!-- Custom CSS -->
    <link href="css/clean-blog.min.css" rel="stylesheet">

    <!-- Custom Fonts -->
    <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href='http://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>

      <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
      <link href="../../assets/css/ie10-viewport-bug-workaround.css" rel="stylesheet">

      <!-- Custom styles for this template -->
      <link href="starter-template.css" rel="stylesheet">

      <!-- Just for debugging purposes. Don't actually copy these 2 lines! -->
      <!--[if lt IE 9]><script src="../../assets/js/ie8-responsive-file-warning.js"></script><![endif]-->
      <script src="../../assets/js/ie-emulation-modes-warning.js"></script>

      <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
      <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
        <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
      <![endif]-->
    </head>

    <body>

      <nav class="navbar navbar-inverse navbar-fixed-top">
        <div class="container">
          <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
              <span class="sr-only">Toggle navigation</span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="#">LANDSAT landsats</a>
          </div>
          <div id="navbar" class="collapse navbar-collapse">
            <ul class="nav navbar-nav">
              <li class="active"></li>
              <li><a href="/about">About</a></li>
              <li><a href="/contact">Contact</a></li>
            </ul>
          </div><!--/.nav-collapse -->
        </div>
      </nav>

  <br><br>

<div class="jumbotron">
<div class="col-lg-4 col-lg-offset-1 col-md-5 col-md-offset-1">
  <h1 style="color:white;">LANDSAT </h1>
    <h1 style="color:white;">landstats</h1>
</div>
</div>

  <article>
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">

    <h3>A population model you can count on</h3>
    <p> 

      The LANDSAT landstats experiment is a supervised learning to predict socio-economic characteristics from satellite data. Specifically, a convolutional neural network to predicts population density from LANDSAT 7 Top Of Atmosphere (TOA) composites. You can easily modify this framwork to employ other models to predict other socio-economic characteristics, using other satellite images. If you're interested in modifying this or in the details, see the <a href="https://github.com/patrick-dd/landsat-landstats">github repository</a>. 
    </p>
    <p>
      LANDSAT landstats aims to augment government service delivery at low cost. The model here was developed within four weeks at <a href="http://www.insightdatascience.com">Insight Data Science</a> NYC in winter 2016. Given the limited time frame and computational resources, the census projections outperform LANDSAT landstats. The experiment provides promise of improving the precision of government statistics and government service delivery.
    </p>
    <p> 
      In the rest of this page, I'll briefly show you what's under the hood. The page is structured as follows:
    </p>

      <ul>
        <li>Why the census</li>
        <li>Data</li>
        <li>Model</li>
        <li>Validation</li>
      </ul>

      <h3>Why the census</h3>

      <p>The census is an important undertaking. The census provides us with an understanding of who we are. We can find out where we are, how we define ourselves and how this changes over time.</p>
      
      <p>
      The results of the United States census allocates approximately $400 billion. The allocation is for various public services such as schools, hospitals, infrastructure and emergency services amongst others. Servicing these needs requires reliable information. This information arrives every decade. A lot of things happen in a decade. Between the 2000 and 2010 censuses Texas' population grew by 4.3 million. Michigan's reduced by 55,000. The demand for public services shifts with these numbers.
      </p>

      <p>
      Between 2000 and 2010 the census department relies on projections. These projections are based on time series estimates of births, deaths and migration. In aggregate, these projects are quite accurate: within three percent of the census estimate. The accuracy by state is also quite good, ranging between -4 and 2 percent. The accuracty diminishes at the county level [fig], with county estimates varying by a 20 percent or more. <a href="#footnote-1">[1]</a> 
    </p>

    <p>
      Since counties deliver many public services, an improved measure is of use. Running the census are more frequent intervals would provide information. Unfortunately the cost is high: the 2010 Census cost $13 billion and the 2020 census could cost up to $18 billion. 
    </p>

    <h3>Data</h3>

    There are two main sources of data: satellite imagery and census population data. 

    <h4> Satellite imagery </h4>
    <p>
    LANDSAT landstats uses a LANDSAT 7 TOA composite. The images are cleaned of noise like clouds so that the model can focus on the ground. <a href="http://www.sciencedirect.com/science/article/pii/S0034425709000169">See Chander et al. (2009)</a> for details. You can download images using the <a href="https://code.earthengine.google.com/">Google Earth Engine</a>, and <a  href="https://github.com/patrick-dd/landsat-landstats/blob/master/ee_data.ipynb">this script</a>. The composites are developed each year from 1999. 
</p>
<p>
    To train the model, 2010 data is used. This coincides with the 2010 census. I use a scale of 100m per pixel and focus on the state of Oregon. <a href="#footnote-1">[2]</a>
</p>
<p>
    LANDSAT 7 images have eight channels, seven of which are available in the TOA composite. These channels measure different light bandwidths. Most of us can see red, blue and green channels with our eyes. In addition to these channels, we have various infra red channels. Each channel provides a difference source of information. We can see this in Figure X, a histogram of intensity at the pixel level.
  </p>
[Histogram here]
  
    <h4> Census data </h4>
    <p>
      The population data comes from the <a href="https://www.census.gov/geo/maps-data/data/tiger-data.html">US census website</a>. I take population data at the census block level, highest spatial resolution possible. The typical census blocks contains between 600 and 3000 people. The area of the average census block is approximately equal to 3 pixels. 
    </p>

<h4> Creating observations </h4>
<p>
  The census data was overlayed onto the satellite image. To create images, the Oregon wide satellite image was convolved into 64 x 64 pixel images. The procedure starts at the north west corner of the image and takes a 64 x 64 pixel observation. Then the window shifts east one pixel. This is another observation. When the window hits the easternmost part of the image, we shift south one pixel, and so on. The procedure prevents connected features like cities from being artificially excluded from images via a simple mesh grid.
</p>

<h4> Stratified sampling </h4>
<p>
    One issue with using satellite data to estimate population is how we distribute ourselves across space. Around 97 percent of pixels are empty of humans. Figure Y shows a histogram of population density across observations. As a species, we like to cluster together.
</p>
[Histogram of pop density]
<p> 
  This high proportion of pixels without value led to poor model performance. A revised strategy sampled based on the whether the area was urban or rural. Weights for sampling were created by first ranking observations by the number of urban pixels (as defined by the US Census office). Then the ranks were inverted and normalised. Figure XX shows a revised distribution.
</p> 

<h3> Model </h3>
<p>

The model used is a convolutional neural network. This class of models has become the state of the art for image classification in recent years. This project investigates how well it can perform for a regression problem.
</p>
<p>
Convolutional neural networks have two advantages. First, as it is a neural network, we do not need to focus on feature engineering. This is of use when we don't have a reliable model of how features generate outputs. Although we can see a correlation between satellite images and population density, it is not clear exactly how these map to each other.
</p>
<p>
A second advantage is that convolutional neural networks retain the spatial structure that occurs accross a landscape. Rather than treating each pixel as independent from each other, convolutional neural networks convolve across the observation. The convolution takes the area around a pixel as an input with the pixel. This way edges, curves and other structures can be used by the network to predict population.
</p>
<p>
The model uses 5 convolutional layers with max pooling and a single feed forward layer before regression. To prevent overfitting, dropout rates of 0.25 were applied at each layer. An original learning rate of 0.2 was used with the X optimizer. Afterwards I manually lowered the learning rate by a magnitude of 10 and used the `adam' optimizer. This process was repeated 2 times.
</p>  

<h3> Results </h3>
<p>
The model is able to explain 26 percent of the variation in population density across the sample in Washington State. Figure X shows the prediction against log normalised actual population density. Points to the left of the graph are lower density regions and points to the right are higher density.
</p>
[Figure X]
<p>
The model performs best in high density regions. We see that the predictions are near to the 45 degree line. True, the line lies underneath the line, perhaps caused by the model trying to also estimate low density areas. A look at the results of an earlier working model with coarser satellite imagery shows a similar shape, but with lower predictions (figure X). This provides evidence that the model is learning. Using shallower models provides similar results. 
</p>
<p>
At low density regions, we have noise. A possible explanation is that the satellite imagery used is too coarse to identify low density areas. Houses in these areas are likely to be further apart and surrounded by nature. At pixel sizes of 100m per pixel, a house is likely to look like empty land.
</p>
<h4> County estimates</h4>
<p>
The outputs above were for a sample of areas in Washington state. To generate County-wide estimates for use in this website, I spatially interpolated between these points. Specifically, I used a k-nearest neighbouts regression algorithm. This algorithm computes a pixel's population as the average population of the k nearest neighbours for which we have predictions. 
</p>
<p>
  There are two details to discuss in implementation. First, outputs are 64x64 pixel images, rather than single pixels. Second, these images may overlap in places. There are two options: make neighbours images, or make neighbours the center of these images. I choose the latter for simplicity.<a href="#footnote-3">[3]</a> 
</p>




      <p id="footnote-2">[2] Downloading images with better resolution results in large image sizes. The Google Earth Engine breaks large images into smaller images, seemingly at random (presumably to lower the strain on the server).
      </p>
      <p id="footnote-3">[3] Sure, sure it's possible stick the pixel representing the image anywhere in the pixel. You could also make a claim that the center is not anymore reasonable than any other area. Good for you :) 
      </p> 
      5 February 2015
            </div>
            </div>
        </div>
    </article>



      

    
      <!-- Bootstrap core JavaScript
      ================================================== -->
      <!-- Placed at the end of the document so the pages load faster -->
      <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
      <script>window.jQuery || document.write('<script src="../../assets/js/vendor/jquery.min.js"><\/script>')</script>
      <script src="../../dist/js/bootstrap.min.js"></script>
      <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
      <script src="../../assets/js/ie10-viewport-bug-workaround.js"></script>
    </body>
  </html>
S